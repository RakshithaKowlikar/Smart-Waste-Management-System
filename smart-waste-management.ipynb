{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11009814,"sourceType":"datasetVersion","datasetId":6854552}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:50:12.036266Z","iopub.execute_input":"2025-03-14T11:50:12.036542Z","iopub.status.idle":"2025-03-14T11:50:19.490631Z","shell.execute_reply.started":"2025-03-14T11:50:12.036512Z","shell.execute_reply":"2025-03-14T11:50:19.489931Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Dataset Path (adjust these paths based on your Kaggle dataset structure)\n#zip_path = '/kaggle/input/garbage-images.zip'  # Corrected zip_path and added .zip extension\nextract_path = '/kaggle/input/garbage-images' #Where to extract to.\n\n# Extract the zip file\n#with zipfile.ZipFile(zip_path, 'r') as zip_ref: # Corrected variable name\n#    zip_ref.extractall(extract_path)\n\n# Correct the paths to the extracted directories\ntrain_dir = os.path.join(extract_path, 'filtered_DATASET', 'TRAIN') #adjust 'train' if needed\ntest_dir = os.path.join(extract_path, 'filtered_DATASET', 'TEST') #adjust 'test' if needed\n\n# Define Transforms\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Custom Dataset Class\nclass WasteDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = os.listdir(root_dir)\n        self.image_paths = []\n        self.labels = []\n\n        for label, class_name in enumerate(self.classes):\n            class_dir = os.path.join(root_dir, class_name)\n            for image_name in os.listdir(class_dir):\n                self.image_paths.append(os.path.join(class_dir, image_name))\n                self.labels.append(label)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Create Datasets and DataLoaders\ntrain_dataset = WasteDataset(train_dir, transform=transform_train)\ntest_dataset = WasteDataset(test_dir, transform=transform_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# EfficientNetV2 Model (simplified, for demonstration)\nclass EfficientNetV2Lite(nn.Module):\n    def __init__(self, num_classes=2):\n        super(EfficientNetV2Lite, self).__init__()\n        self.features = torchvision.models.efficientnet_v2_s(pretrained=True).features\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(1280, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Initialize Model, Loss Function, and Optimizer\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = EfficientNetV2Lite().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n\n# Evaluation\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy on test set: {100 * correct / total}%\")\n\n# Saving model to Kaggle's working directory\ntorch.save(model.state_dict(), '/kaggle/working/waste_classification_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T11:50:19.491526Z","iopub.execute_input":"2025-03-14T11:50:19.491914Z","iopub.status.idle":"2025-03-14T12:31:26.794072Z","shell.execute_reply.started":"2025-03-14T11:50:19.491890Z","shell.execute_reply":"2025-03-14T12:31:26.792873Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 205MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.25075830444268427\nEpoch 2/10, Loss: 0.20541344153642996\nEpoch 3/10, Loss: 0.1865541626607598\nEpoch 4/10, Loss: 0.1691878385198822\nEpoch 5/10, Loss: 0.1575725225468675\nEpoch 6/10, Loss: 0.14068830512814787\nEpoch 7/10, Loss: 0.13099571716003172\nEpoch 8/10, Loss: 0.11703787733283898\nEpoch 9/10, Loss: 0.10996147847406236\nEpoch 10/10, Loss: 0.09270563891779275\nAccuracy on test set: 91.70323095333067%\n","output_type":"stream"}],"execution_count":2}]}